{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af7db82-57b2-400b-b26e-d1af25cd3e11",
   "metadata": {},
   "source": [
    "# Data collection (Financial data only)\n",
    "\n",
    "The very first step is to collect data!\n",
    "The data will be collected and saved in three folders:\n",
    "- #### End of day data\n",
    "Historical end of day data for stock prices.\n",
    "\n",
    "- #### Fundamental data\n",
    "Fundamental data for a given stock i.e eps and outstanding shares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969f4761-97d0-4044-867b-dcea5ad27e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import nasdaqdatalink\n",
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "from eodhd import APIClient\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d0562c-ee51-4b6c-9b1f-93b36419db6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = \"64d77f6d3a60a5.24835840\" #API key\n",
    "eod_path = Path.cwd()/\"Eod\" #path to create folder where end of day stock prices will be stored\n",
    "fundamentals_path = Path.cwd()/\"Fundamentals\"\n",
    "SingleStock_data = Path.cwd()/\"SingleStock_data\"\n",
    "Path.mkdir(eod_path, exist_ok=True)\n",
    "Path.mkdir(fundamentals_path, exist_ok = True)\n",
    "Path.mkdir(SingleStock_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff22e5f-7449-4209-8f84-d1cc526929f9",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5da3d15-4111-4181-8eb9-20a65d31cf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stock_eod(ticker):\n",
    "    \"\"\"\n",
    "    Get stock data End of Day prices.\n",
    "    \n",
    "    Args(str): ticker name of the stock\n",
    "    \n",
    "    Returns a pandas dataframe outer-merged on 'datetime' \n",
    "    \"\"\"\n",
    "    \n",
    "    ###MAKE API CLALL###\n",
    "    Timeseries = requests.get(f'https://eodhistoricaldata.com/api/eod/{ticker}.US?order=d&from=2022-01-01&to=2023-07-31&period=d&&api_token={api}&fmt=json')\n",
    "    \n",
    "    ### convert json files to pandas dataframes\n",
    "    timeseries_df = pd.DataFrame(Timeseries.json())\n",
    "    timeseries_df[\"date\"] = pd.to_datetime(timeseries_df[\"date\"])  #change date format to datetime\n",
    "    \n",
    "    stock_df = timeseries_df\n",
    "    \n",
    "    return stock_df\n",
    "\n",
    "def stock_fundamental(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Collect fundamentals for a stock given its ticker\n",
    "    \n",
    "    Args(str): ticker\n",
    "    \n",
    "    Returns a pandas dataframe with fundamentals data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Get all fundamental data for a given ticker \n",
    "    url = f'https://eodhistoricaldata.com/api/fundamentals/{ticker}.US?api_token={api}&order=d&filter=outstandingShares::quarterly,Earnings::History'\n",
    "    resp = requests.get(url) #make request\n",
    "    json = resp.json() #convert to json\n",
    "\n",
    "    #get outstandingShares data and convert to dataframe\n",
    "    out = pd.DataFrame(json[\"outstandingShares::quarterly\"]).T\n",
    "    out = out.drop([\"date\", \"sharesMln\"], axis = 1).rename(columns = {\"dateFormatted\":\"date\", \"shares\":\"outstandingShares\"})\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"]) #change dateformat to datetime\n",
    "    out.set_index('date', inplace =True)\n",
    "    out = out.resample('D').ffill() #Fill values in between days\n",
    "    out.sort_index(ascending=False, inplace=True)\n",
    "    out.reset_index(inplace=True)\n",
    "\n",
    "    #get earnings data and convert to dataframe\n",
    "    earn = pd.DataFrame(json[\"Earnings::History\"]).T\n",
    "    earn = earn.drop([\"reportDate\", \"beforeAfterMarket\"], axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    earn[\"date\"] = pd.to_datetime(earn[\"date\"]) #change dateformat to datetime\n",
    "    earn.set_index('date', inplace =True)\n",
    "    earn = earn.resample('D').ffill() #Fill values in between days\n",
    "    earn.sort_index(ascending=False, inplace=True)\n",
    "    earn.reset_index(inplace=True)\n",
    "\n",
    "    fundamental = earn.merge(out, how = \"left\", on = \"date\")\n",
    "    fundamental = fundamental[(fundamental[\"date\"]<='2023-07-31') & (fundamental[\"date\"]>='2022-01-01')].reset_index()\n",
    "    \n",
    "    #write a status to see if all data was gotten. Used in logfile later\n",
    "    status = \"No\"\n",
    "    if fundamental.shape == (577,8):\n",
    "        status = \"Yes\"\n",
    "    else:\n",
    "        status = \"No\"\n",
    "    \n",
    "    return fundamental, status\n",
    "\n",
    "\n",
    "def df_to_csv(df, name):\n",
    "    \"\"\"\n",
    "    Save a pandas dataframe into a csv file\n",
    "    \n",
    "    Args(pd.DataFrame, str): dataframe and the name of the file it should return.\n",
    "    When working with stock data name should be the ticker of the specified stock.\n",
    "    \n",
    "    requirements = requests, pandas, json should be installed and imported.\n",
    "    \n",
    "    returns 0, creates a csv file in /Data/Financial data\n",
    "    \"\"\"\n",
    "\n",
    "    pathname = Path.cwd()/f\"SingleStock_data/{name}.csv\"\n",
    "    \n",
    "    df.to_csv(pathname, index = False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def log(ticker, df, logfile, accept,output_path=os.getcwd()):\n",
    "    #open or create the csv file\n",
    "    if os.path.isfile(logfile): #if log file exist, open and allow changes\n",
    "        log = open(logfile,'a')\n",
    "    else:\n",
    "        log = open(logfile,'w')\n",
    "        header = ['Timestamp', 'Company', 'shape', 'Accept']\n",
    "        log.write(\";\".join(header)+\"\\n\") #Make the headers and jump to the new line\n",
    "    \n",
    "    #Gather log information\n",
    "    company = f\"{ticker}\"\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #local time\n",
    "    shape = df.shape\n",
    "    \n",
    "    #Open the log file and append the gathered log information\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(f'{timestamp};{company};{shape};{accept}' + \"\\n\") #Append the information and jump to the new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59160e-560e-4fce-acfc-9c86ccd3dc81",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d58f2e39-ddf0-43d1-8015-d19c07af0251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2188"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list with all US-listed companies in NASDAQ exchange over 50m market cap\n",
    "cwd = Path.cwd() #current working directory\n",
    "nasdaq = pd.read_csv(\"Nasdaq mkt. cap 50m+.csv\")\n",
    "nasdaq = nasdaq.sort_values(\"Market Cap\", ascending = False).reset_index().drop(\"index\", axis = 1)\n",
    "tickers = nasdaq[\"Symbol\"].values\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eec7992-3a4f-4cd3-907d-b9f978cf132e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#User new list of tickers instead\n",
    "#Note: this block was made after the whole code below was run for the first time. We had to get the data all over again and removed some of the original tickers shown above\n",
    "\n",
    "tickers = pd.read_csv(\"Tickers_final.csv\")[\"Ticker\"].values\n",
    "tickers = tickers\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "998a05fd-8e27-451f-ac6f-141fe46ea9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022it [1:13:31,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logfile = Path.cwd()/f'financial_data_logs.csv'\n",
    "missing = []\n",
    "index_missing = []\n",
    "fundamentals_failed = []\n",
    "fundamentals_index = []\n",
    "\n",
    "for i, ticker in tqdm.tqdm(enumerate(tickers)):\n",
    "    try:\n",
    "        #Get End-of-Day stock price data\n",
    "        eod = stock_eod(ticker)\n",
    "        #Get fundamentals\n",
    "        fund, status = stock_fundamental(ticker)\n",
    "        if status == \"No\":\n",
    "            fundamentals_failed.append(ticker)\n",
    "            fundamentals_index.append(ticker)\n",
    "        \n",
    "        try:\n",
    "            eod.to_csv(cwd/f'Eod/{ticker}.csv', index = False)  #save end of day data only \n",
    "            fund.to_csv(cwd/f'Fundamentals/{ticker}.csv', index = False)  #save fundamental data only\n",
    "\n",
    "            #merge fundamental and end of day data on date\n",
    "            stock = eod.merge(fund, how = \"left\", on = \"date\")\n",
    "            df_to_csv(stock,ticker) #save all data for the single stock\n",
    "            log(ticker, stock, logfile, status) #track all succeded logs\n",
    "        except:\n",
    "            print(f\"Data for company {ticker} was retrieved from API but not saved\")\n",
    "                \n",
    "    except:\n",
    "        print(f'Company {ticker} was not retrived from API')\n",
    "        missing.append(ticker)\n",
    "        index_missing.append(i)\n",
    "        time.sleep(5)\n",
    "            \n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67a16cb-5eec-4d75-9af8-183151b98d5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([missing, index_missing])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      2\u001b[0m fundamentals_missing \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([fundamentals_failed,fundamentals_index])\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missing' is not defined"
     ]
    }
   ],
   "source": [
    "missing_values = pd.DataFrame([missing, index_missing]).T\n",
    "fundamentals_missing = pd.DataFrame([fundamentals_failed,fundamentals_index]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff19434-2229-443c-8441-a9597f92b736",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fundamentals_missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fundamentals_missing\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fundamentals_missing' is not defined"
     ]
    }
   ],
   "source": [
    "fundamentals_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550c8c6-a6f2-4a4f-a6a4-ce45460e7523",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validate datacollection\n",
    "As shown above there are 70 stocks that could not be retrieved. After some consideration it was decided to drop these stocks because of two reason.\n",
    "Some of them are just to small or were listed after 2023-06-01, meaning there will be some missing data.\n",
    "\n",
    "The last argument needs to be validated for all the stocks there is data on, as if some stocks have less available dates or na values then they should be reevaluated.\n",
    "\n",
    "Therefore, the next block will be about evaluation of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853e8dc3-2d66-4552-87ad-b87e21afb1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a validation log that saves the information of all stocks\n",
    "def log_val(ticker, df, logfile, output_path=os.getcwd()):\n",
    "    #open or create the csv file\n",
    "    if os.path.isfile(logfile): #if log file exist, open and allow changes\n",
    "        log = open(logfile,'a')\n",
    "    else:\n",
    "        log = open(logfile,'w')\n",
    "        header = ['Timestamp', 'Ticker', 'Shape', 'Accept', 'Nr. NaN values']\n",
    "        log.write(\";\".join(header)+\"\\n\") #Make the headers and jump to the new line\n",
    "    \n",
    "    accept = \"Yes\"\n",
    "    accept_cond = (267,14)\n",
    "    #Gather log information\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #local time\n",
    "    shape = df.shape\n",
    "    if df.shape != accept_cond:\n",
    "        accept = \"No\"\n",
    "    nan_val = sum(df.isna().sum().values)\n",
    "    \n",
    "    #Open the log file and append the gathered log information\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(f'{timestamp};{ticker};{shape};{accept};{nan_val}' + \"\\n\") #Append the information and jump to the new line.\n",
    "        \n",
    "#create a validation log that saves the information of all stocks fundamentals\n",
    "def log_val_fund(ticker, df, logfile, output_path=os.getcwd()):\n",
    "    #open or create the csv file\n",
    "    if os.path.isfile(logfile): #if log file exist, open and allow changes\n",
    "        log = open(logfile,'a')\n",
    "    else:\n",
    "        log = open(logfile,'w')\n",
    "        header = ['Timestamp', 'Ticker', 'Shape', 'Accept', 'Nr. NaN values']\n",
    "        log.write(\";\".join(header)+\"\\n\") #Make the headers and jump to the new line\n",
    "    \n",
    "    accept = \"Yes\"\n",
    "    accept_cond = (365,8) #the shape it should have in the research period\n",
    "    #Gather log information\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #local time\n",
    "    shape = df.shape\n",
    "    if df.shape != accept_cond:\n",
    "        accept = \"No\"\n",
    "    nan_val = sum(df.isna().sum().values)\n",
    "    \n",
    "    #Open the log file and append the gathered log information\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(f'{timestamp};{ticker};{shape};{accept};{nan_val}' + \"\\n\") #Append the information and jump to the new line.\n",
    "\n",
    "#create a validation log that saves the information of all stocks eod\n",
    "def log_val_eod(ticker, df, logfile, output_path=os.getcwd()):\n",
    "    #open or create the csv file\n",
    "    if os.path.isfile(logfile): #if log file exist, open and allow changes\n",
    "        log = open(logfile,'a')\n",
    "    else:\n",
    "        log = open(logfile,'w')\n",
    "        header = ['Timestamp', 'Ticker', 'Shape', 'Accept', 'Nr. NaN values']\n",
    "        log.write(\";\".join(header)+\"\\n\") #Make the headers and jump to the new line\n",
    "    \n",
    "    accept = \"Yes\"\n",
    "    accept_cond = (267,7)\n",
    "    #Gather log information\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #local time\n",
    "    shape = df.shape\n",
    "    if df.shape != accept_cond:\n",
    "        accept = \"No\"\n",
    "    nan_val = sum(df.isna().sum().values)\n",
    "    \n",
    "    #Open the log file and append the gathered log information\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(f'{timestamp};{ticker};{shape};{accept};{nan_val}' + \"\\n\") #Append the information and jump to the new line.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ea071b3-302c-4e8e-a68b-7d6184fe9088",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'missing_stocks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tickers \u001b[38;5;66;03m#all tickers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_stocks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m idxs \u001b[38;5;241m=\u001b[39m missing_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'missing_stocks.csv'"
     ]
    }
   ],
   "source": [
    "tickers #all tickers\n",
    "missing_values = pd.read_csv(\"missing_stocks.csv\")\n",
    "idxs = missing_values[\"1\"].values #index for missing companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe59a9-1e52-40c9-bac1-a788d7dd608f",
   "metadata": {},
   "source": [
    "#### Validation of Eod data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386aa53b-4cb8-4f91-a678-d3088df22060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl = pd.read_csv(eod_path/\"AAPL.csv\")\n",
    "aapl[\"date\"]=pd.to_datetime(aapl[\"date\"])\n",
    "aapl[(aapl[\"date\"]>'2022-05-10') & (aapl[\"date\"]<'2023-06-03')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9fb6079-29ec-47c1-b18b-b2425a88f903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right shape for at stock is (395, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2022/2022 [00:07<00:00, 280.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get data for each stock and check it has the right shape. The right shape will be the same shape as apple stock has (has date on all cells)\n",
    "aapl = pd.read_csv(eod_path/\"AAPL.csv\")\n",
    "print(f\"The right shape for at stock is {aapl.shape}\")\n",
    "logval = Path.cwd()/f'Stock_validation_eod.csv'\n",
    "\n",
    "for i in tqdm.tqdm(range(len(tickers))):\n",
    "    #check if i stock is one of the 70 missing\n",
    "    \n",
    "    df = pd.read_csv(eod_path/f\"{tickers[i]}.csv\")\n",
    "    df[\"date\"]=pd.to_datetime(df[\"date\"])\n",
    "    df = df[(df[\"date\"]>'2022-05-10') & (df[\"date\"]<'2023-06-03')]\n",
    "    log_val_eod(tickers[i], df, logval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d34573-7f9b-4d5e-a540-ee30e6466920",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The commented code below is used in case some companies as missing - this is no longer the case after data extraction number 2\n",
    "'''    \n",
    "abval = 0\n",
    "for j in range(len(idxs)):\n",
    "        if i==0:\n",
    "            continue\n",
    "        elif i==idxs[j]:\n",
    "            abval = 1\n",
    "            #print(\"Missing company found with index: \",i)\n",
    "    if abval == 1:\n",
    "        continue'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fcded9a-93df-46d9-a312-2eb9336ef346",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MOND', 'LIFW', 'IVCP', 'LCA', 'BGXX', 'FXNC', 'CSBR', 'ITAQ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get log-validation file created above\n",
    "validation_eod = pd.read_csv(\"Stock_validation_eod.csv\", sep = \";\")\n",
    "\n",
    "#Find all stocks that were not accepted due to data shortage\n",
    "eod_del = validation_eod[validation_eod[\"Accept\"]==\"No\"][\"Ticker\"].values\n",
    "\n",
    "'''if sum(eod_del)==0:\n",
    "    print(f\"There's complete data for all {len(tickers)} companies!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0a7e59c-a001-4efb-a5db-5d4cbd9c34a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118/2118 [00:00<00:00, 52067.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now we have 70 companies that could not be retrieved for API and 96 that were listed after the given date period.\n",
    "#Its time to remove these companies from the dataset\n",
    "\n",
    "#Remove Non-retrieved-data from tickers list:\n",
    "tickers_new = np.delete(tickers, idxs)\n",
    "\n",
    "#Remove tickers with incomple datasets from tickers_list\n",
    "index_eod_remove = []\n",
    "for i in tqdm.tqdm(range(len(tickers_new))):\n",
    "    for j in range(len(eod_del)):\n",
    "        if tickers_new[i] == eod_del[j]:\n",
    "            index_eod_remove.append(i)\n",
    "tickers_final = np.delete(tickers_new, index_eod_remove)\n",
    "len(tickers_final)\n",
    "\n",
    "#Save as csv file\n",
    "pd.DataFrame(tickers_final, columns = [\"Ticker\"]).to_csv(\"Tickers_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3f64384-b27e-4ef4-a408-970d55ed3252",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118/2118 [00:00<00:00, 15272.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eod's removed:  96\n",
      "Fundamentals's removed:  96\n",
      "StockSingle's removed:  96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Now its time to remove \n",
    "#as this code is a bit dangerous i commented it\n",
    "#THINK CAREFULLY before using!!! it will delete a lot of data\n",
    "\n",
    "'''eod_succed = []\n",
    "fundamentals_succed = []\n",
    "StockSingle_succed = []\n",
    "check = []\n",
    "\n",
    "#Delete all files from stocks with data less than one year\n",
    "for ticker in tqdm.tqdm(tickers_new):\n",
    "    for j in range(len(eod_del)):\n",
    "        if ticker==eod_del[j]:\n",
    "            check.append(ticker)\n",
    "            if os.path.exists(eod_path/f'{ticker}.csv'):\n",
    "                os.remove((eod_path/f'{ticker}.csv'))\n",
    "                eod_succed.append(1)\n",
    "            else:\n",
    "                eod_succed.append(0)\n",
    "\n",
    "            if os.path.exists(fundamentals_path/f'{ticker}.csv'):\n",
    "                os.remove((fundamentals_path/f'{ticker}.csv'))\n",
    "                fundamentals_succed.append(1)\n",
    "            else:\n",
    "                fundamentals_succed.append(0)\n",
    "\n",
    "            if os.path.exists(SingleStock_data/f'{ticker}.csv'):\n",
    "                os.remove((SingleStock_data/f'{ticker}.csv'))\n",
    "                StockSingle_succed.append(1)\n",
    "            else:\n",
    "                StockSingle_succed.append(0)\n",
    "\n",
    "print(\"Eod's removed: \",sum(eod_succed))\n",
    "print(\"Fundamentals's removed: \",sum(fundamentals_succed))\n",
    "print(\"StockSingle's removed: \",sum(StockSingle_succed))'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886b39a-032c-4658-bbc3-43d2fbc65f52",
   "metadata": {},
   "source": [
    "#### Validation of fundamental data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a8b972f-847b-4fca-89af-5f642a41788c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right shape for at stock is (365, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2022/2022 [00:08<00:00, 238.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#tickers_final = np.delete(tickers, idxs)\n",
    "aapl = pd.read_csv(fundamentals_path/\"AAPL.csv\")\n",
    "print(f\"The right shape for at stock is {aapl.shape}\")\n",
    "logval = Path.cwd()/f'Stock_validation_fundamentals.csv'\n",
    "\n",
    "for i in tqdm.tqdm(range(len(tickers))):\n",
    "    df = pd.read_csv(fundamentals_path/f\"{tickers[i]}.csv\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[(df[\"date\"]>'2022-06-01')&(df[\"date\"]<'2023-06-02')]\n",
    "    log_val_fund(tickers[i], df, logval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "abad1117-e588-422c-8622-a1fd23e98f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Nr. NaN values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2023-08-17 11:04:20</td>\n",
       "      <td>SABRP</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2023-08-17 11:04:20</td>\n",
       "      <td>HBANP</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2023-08-17 11:04:20</td>\n",
       "      <td>FITBI</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2023-08-17 11:04:20</td>\n",
       "      <td>FITBP</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2023-08-17 11:04:20</td>\n",
       "      <td>AGNCN</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2023-08-17 11:04:28</td>\n",
       "      <td>BAFN</td>\n",
       "      <td>(303, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2023-08-17 11:04:28</td>\n",
       "      <td>MOVE</td>\n",
       "      <td>(303, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2023-08-17 11:04:28</td>\n",
       "      <td>DUNE</td>\n",
       "      <td>(303, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2023-08-17 11:04:28</td>\n",
       "      <td>DUNEU</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2023-08-17 11:04:28</td>\n",
       "      <td>NSTS</td>\n",
       "      <td>(303, 8)</td>\n",
       "      <td>No</td>\n",
       "      <td>909.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp Ticker     Shape Accept  Nr. NaN values\n",
       "94    2023-08-17 11:04:20  SABRP    (0, 8)     No             0.0\n",
       "95    2023-08-17 11:04:20  HBANP    (0, 8)     No             0.0\n",
       "125   2023-08-17 11:04:20  FITBI    (0, 8)     No             0.0\n",
       "135   2023-08-17 11:04:20  FITBP    (0, 8)     No             0.0\n",
       "148   2023-08-17 11:04:20  AGNCN    (0, 8)     No             0.0\n",
       "...                   ...    ...       ...    ...             ...\n",
       "1979  2023-08-17 11:04:28   BAFN  (303, 8)     No           543.0\n",
       "1994  2023-08-17 11:04:28   MOVE  (303, 8)     No           545.0\n",
       "1996  2023-08-17 11:04:28   DUNE  (303, 8)     No           907.0\n",
       "1997  2023-08-17 11:04:28  DUNEU    (0, 8)     No             0.0\n",
       "2020  2023-08-17 11:04:28   NSTS  (303, 8)     No           909.0\n",
       "\n",
       "[138 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get log-validation file created above\n",
    "validation_fund = pd.read_csv(\"Stock_validation_fundamentals.csv\", sep = \";\")\n",
    "\n",
    "#Save a variable containing the tickers of datasets that were not accepted (Shape != (368,8))\n",
    "BadShape = validation_fund[validation_fund[\"Accept\"]==\"No\"]\n",
    "BadShape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63834cdd-2e3c-42a3-989b-3481b69e2719",
   "metadata": {},
   "source": [
    "### Single Stock data (merged date) validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c5f18ef5-2118-4228-b0e9-ba5c232bafea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right shape for at stock is (267, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2022/2022 [00:09<00:00, 216.05it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The right shape for at stock is {aapl.shape}\")\n",
    "logval = Path.cwd()/f'Stock_validation_merged.csv'\n",
    "\n",
    "for i in tqdm.tqdm(range(len(tickers))):\n",
    "    df = pd.read_csv(SingleStock_data/f\"{tickers[i]}.csv\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[(df[\"date\"]>'2022-05-10')&(df[\"date\"]<'2023-06-03')]\n",
    "    log_val(tickers[i], df, logval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "44fd0de3-8c37-47d5-b1ba-ccfb6591aab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get log-validation file created above\n",
    "validation_merge = pd.read_csv(\"Stock_validation_merged.csv\", sep = \";\")\n",
    "\n",
    "#Save a variable containing the tickers of datasets that were not accepted (Shape != (368,8))\n",
    "BadShape = validation_merge[validation_merge[\"Accept\"]==\"No\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a43a857d-63de-42e3-b23f-22f23a3640de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    2014\n",
       "No        8\n",
       "Name: Accept, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_merge[\"Accept\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8010b-c9ca-48a1-b833-20d4b5fe1b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
